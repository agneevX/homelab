version: "3.9"

x-env: &env
  PGID: 1001
  PUID: 1001
  UMASK: 002
  TZ: Asia/Kolkata

x-labels: &pullio
  org.hotio.pullio.update: true
  org.hotio.pullio.notify: true
  org.hotio.pullio.discord.webhook: $DISCORD_WEBHOOK

volumes:
  netdata-lib:
    name: netdata-lib
  netdata-cache:
    name: netdata-cache

networks:
  default:
    name: skynet
    ipam:
      config:
        - subnet: 172.18.0.0/16
          gateway: 172.18.0.1

services:

#   tailscale:
#     container_name: tailscale
#     image: jauderho/tailscale
#     restart: always
#     hostname: falcon
#     network_mode: host
#     privileged: true
#     command: tailscaled
#     volumes:
#      - /var/lib/tailscale:/var/lib/tailscale
#      - /dev/net/tun:/dev/net/tun
#     labels:
#      <<: *pullio

#  cloudflared:
#    container_name: cloudflared
#    image: visibilityspots/cloudflared
#    restart: unless-stopped
#    environment:
#      <<: *env
#    labels:
#      <<: *pullio

  autoheal:
    container_name: autoheal
    image: willfarrell/autoheal
    restart: always
    environment:
      AUTOHEAL_CONTAINER_LABEL: all
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    labels:
      <<: *pullio

  traefik:
    container_name: traefik
    image: traefik
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "traefik", "healthcheck"]
    extra_hosts:
      - "host.docker.internal:10.0.0.11"
    ports:
      - 80:80
    environment:
      <<: *env
    volumes:
      - /opt/appdata/traefik:/etc/traefik
      - /var/run/docker.sock:/var/run/docker.sock:ro
    labels:
      traefik.http.routers.traefik.service: api@internal
      <<: *pullio
 
  dozzle:
    container_name: dozzle
    image: amir20/dozzle
    restart: unless-stopped
    environment:
      <<: *env
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
    labels:
      <<: *pullio

  caddy:
    container_name: caddy
    image: caddy
    restart: unless-stopped
    environment:
      <<: *env
    tmpfs:
      - /data:size=10m
      - /config:size=10m
    volumes:
      - /opt/appdata/caddy/Caddyfile:/etc/caddy/Caddyfile
      - /opt/data/ariang:/ariang
      - /opt/appdata/tandoor/media:/tandoor
    labels:
      <<: *pullio

#   photoprism:
#     container_name: photoprism
#     image: photoprism/photoprism
#     restart: unless-stopped
#     healthcheck:
#       test: ["CMD", "wget", "localhost", "-nv", "-t1", "--spider"]
#     security_opt:
#       - seccomp:unconfined
#       - apparmor:unconfined
#     working_dir: "/photoprism"
#     volumes:
#       - /opt/appdata/photoprism:/photoprism/storage
#       - /mnt/knox/photos:/photoprism/originals
#     environment:
#       PHOTOPRISM_UID: 1001
#       PHOTOPRISM_GID: 1001
#       PHOTOPRISM_DATABASE_DRIVER: "sqlite"
    # labels:
    #   <<: *pullio

  archivebox:
    container_name: archivebox
    image: archivebox/archivebox:master
    restart: unless-stopped
    environment:
      MEDIA_MAX_SIZE: 750m
      <<: *env
    volumes:
      - /opt/appdata/archivebox:/data
    labels:
      <<: *pullio

  autoscan:
    container_name: autoscan
    image: hotio/autoscan
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "localhost:3030", "-so", "/dev/null"]
    ports:
      - 100.120.109.59:3030:3030
    environment:
      <<: *env
    volumes:
      - /opt/appdata/autoscan:/config
      - /mnt/mfs-drive:/drive
      - /mnt/knox:/knox:ro
    labels:
      <<: *pullio

  plextraktsync:
    container_name: plextraktsync
    image: ghcr.io/taxel/plextraktsync
    restart: unless-stopped
    user: "1001:1001"
    command: "watch"
    environment:
      <<: *env
    volumes:
      - /opt/appdata/plextraktsync:/app/config
      - /dev/null:/app/config/last_update.log
    labels:
      <<: *pullio
      
  raspotify:
    container_name: raspotify 
    image: derkades/raspotify
    restart: unless-stopped
    volumes:
      - ~/.raspotify-entrypoint.sh:/startup.sh
    devices:
      - /dev/snd:/dev/snd
    labels:
      <<: *pullio

#   spotifyd:
#     container_name: spotifyd
#     image: zewelor/spotifyd
#     volumes:
#       - /opt/appdata/spotifyd/spotifyd.conf:/etc/spotifyd.conf

# Trials and tribulations of getting a self-hosted Spotify Connect client 
# working in Docker with alsa support
#
# Issues with raspotify images (based on librespot):
# GENERAL ISSUES: Plaintext Spotify login details passed via CLI/mitigated 
# with custom entrypoint script for the derkades image
#
# svanscho/rpi-spotify: ancient version of librespot, has alsa and works, no ap port option, version info
# flaviostutz/rpi-spotify: newer than the above one, lacks features from librespot, has 
# ...: Don't use --mixer=alsa, fails for whatever reason
# derkades/raspotify: best of the lot, but entrypoint needs to be overriden
#
# Issues with librespot images: # Latest librespot version: 0.3.1
# GENERAL ISSUES: Same as raspotify, but most pure 
# librespot images lack alsa support, so audio is very distorted
#
# dubodubonduponey/librespot: above, 
# mazzolino/librespot-snapserver: above, v0.2.0 of librespot
# dwinks/librespot_docker: ??, old version (v0.20.0)
#
# Issues with spotifyd images:
# GENERAL ISSUES: No spaces in device name, issues with alsa playback
#
# hvalev/spotifyd-alsa: permission error, gh issue created
# ggoussard/spotifyd: exec error: no file or directory, unable to create issue on GH: issues disabled
# zewelor/spotifyd: alsa issues, aplay -L, default config options don't work
#

  hdidle:
    container_name: hdidle
    image: hotio/hdidle
    restart: unless-stopped
    environment:
      ARGS: "-i 600"
      <<: *env
    tmpfs:
      - /config:size=1m
    devices:
      - /dev/sdb:/dev/sdb
      - /dev/sdc:/dev/sdc
      - /dev/sdd:/dev/sdd
      - /dev/sde:/dev/sde
    labels:
      <<: *pullio
      
  filebrowser:
    image: filebrowser/filebrowser
    container_name: filebrowser
    restart: unless-stopped
    user: "1001:1001"
    command:
      - --disable-exec
      - --disable-type-detection-by-header
      - --disable-preview-resize
      - --disable-thumbnails
    environment:
      FB_ROOT: /files
      FB_CONFIG: /srv/config.json
      FB_DATABASE: /srv/filebrowser.db
    volumes:
      - /opt/appdata/filebrowser:/srv
      - /opt/appdata:/files/appdata
      - /home/agneev:/files
      - /mnt:/mnt
    labels:
      traefik.http.routers.filebrowser.rule: Host(`files.falcon.nt`)
      <<: *pullio

#   grafana:
#     image: grafana/grafana
#     container_name: grafana
#     restart: unless-stopped
#     healthcheck:
#       test: ["CMD", "wget", "-nv", "-t1", "--spider", "localhost:3000"]
#     environment:
#       GF_AUTH_DISABLE_LOGIN_FORM: true
#       GF_AUTH_ANONYMOUS_ENABLED: true
#       GF_AUTH_ANONYMOUS_ORG_ROLE: Admin
#       GF_SECURITY_ALLOW_EMBEDDING: true
#     volumes:
#       - /opt/appdata/grafana/config:/etc/grafana
#       - /opt/appdata/grafana/data:/var/lib/grafana
#     labels:
#       <<: *pullio

#   prometheus:
#     image: prom/prometheus
#     container_name: prometheus
#     restart: unless-stopped
#     command: 
#       - "--log.level=warn"
#       - "--config.file=/etc/prometheus/prometheus.yml"
#     healthcheck:
#       test: ["CMD", "wget", "-nv", "-t1", "--spider", "localhost:9090"]
#     volumes:
#       - /opt/appdata/prometheus/config:/etc/prometheus
#       - /opt/appdata/prometheus/data:/prometheus
#     labels:
#       <<: *pullio

  shairport-sync:
    container_name: shairport-sync
    image: mikebrady/shairport-sync
    restart: unless-stopped
    network_mode: host
    volumes:
      - /opt/appdata/shairport-sync/config.conf:/etc/shairport-sync.conf
    devices:
      - /dev/snd:/dev/snd
    labels:
      <<: *pullio

  freshrss:
    image: linuxserver/freshrss
    container_name: freshrss
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "localhost", "-fso", "/dev/null"]
    environment:
      <<: *env
    volumes:
      - /opt/appdata/freshrss:/config
    labels:
      <<: *pullio

  netdata:
    image: netdata/netdata
    container_name: netdata
    hostname: falcon
    restart: unless-stopped
    healthcheck:
      test: "/usr/sbin/health.sh > /dev/null 2>&1"
    logging:
      driver: none
    cap_add:
      - SYS_PTRACE
    security_opt:
      - apparmor:unconfined
    ports:
      - 19999:19999
    environment:
      PGID: 1001
      NETDATA_CLAIM_TOKEN: $NETDATA_CLAIM_TOKEN
      NETDATA_CLAIM_URL: https://app.netdata.cloud
      NETDATA_CLAIM_ROOMS: $NETDATA_CLAIM_ROOMS
    volumes:
      - /opt/appdata/netdata:/etc/netdata
      - netdata-lib:/var/lib/netdata
      - netdata-cache:/var/cache/netdata
      - /var/run/docker.sock:/var/run/docker.sock:ro
      # OS access
      - /etc/passwd:/host/etc/passwd:ro
      - /etc/group:/host/etc/group:ro
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /etc/os-release:/host/etc/os-release:ro
    labels:
      <<: *pullio

  yacht:
    container_name: yacht
    image: selfhostedpro/yacht:devel
    restart: always
    healthcheck:
      test: ["CMD", "wget", "-nv", "-t1", "--spider", "localhost:8000"]
    environment:
      TZ: Asia/Kolkata
      SECRET_KEY: key
      DISABLE_AUTH: True
      <<: *env
    volumes:
      - /opt/appdata/yacht:/config
      - /var/run/docker.sock:/var/run/docker.sock
    labels:
      <<: *pullio
  
  scrutiny:
    container_name: scrutiny
    image: hotio/scrutiny
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "localhost:8080", "-fso", "/dev/null"]
    logging:
      driver: none
    cap_add:
      - SYS_RAWIO
    environment:
      INTERVAL: 10800
      <<: *env
    volumes:
      - /run/udev:/run/udev:ro
      - /opt/appdata/scrutiny:/config
    devices:
      - /dev/sda:/dev/sda:ro
      - /dev/sdb:/dev/sdb:ro
      - /dev/sdc:/dev/sdc:ro
      - /dev/sdd:/dev/sdd:ro
      - /dev/sde:/dev/sde:ro
    labels:
      <<: *pullio
      
  node-red:
    container_name: node-red
    image: nodered/node-red
    restart: unless-stopped
    user: "1001:1001"
    environment:
      <<: *env
    volumes:
      - /opt/appdata/node-red:/data
    labels:
      <<: *pullio

  code-server:
    container_name: code-server
    image: linuxserver/code-server
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "localhost:8443", "-fso", "/dev/null"]
    environment:
      <<: *env
    volumes:
      - /opt/appdata/code-server:/config
      - /home/agneev/git:/config/workspace/git
      - /home/agneev/scripts:/config/workspace/scrips
      - /opt/appdata/home-assistant:/config/workspace/ha-config
    labels:
      traefik.http.routers.code-server.rule: Host(`code.falcon.nt`)
      <<: *pullio
 
  home-assistant:
    container_name: home-assistant
    image: homeassistant/home-assistant
    restart: unless-stopped
    network_mode: host
    healthcheck:
      test: ["CMD", "curl", "localhost:8123", "-fso", "/dev/null"]
    environment:
      <<: *env
    volumes:
      - /opt/appdata/home-assistant:/config
      - /run/mysqld/mysqld.sock:/run/mysqld/mysqld.sock
    labels:
      traefik.http.services.home-assistant.loadbalancer.server.port: 8123
      <<: *pullio

  qbittorrent:
    container_name: qbittorrent
    image: hotio/qflood
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "localhost:8080", "-fso", "/dev/null"]
    environment:
      FLOOD_AUTH: true
      <<: *env
    volumes:
      - /opt/appdata/qbittorrent:/config
      - /home/agneev/qbittorrent:/downloads
      - /home/agneev/drive-local/local/qbt:/drive/local/qbt
      - /mnt/knox:/knox
      - /home/agneev/scripts/qbt_post_dl.sh:/scripts/qbt_post_dl.sh:ro
    labels:
      traefik.http.routers.qbittorrent.service: qbittorrent
      traefik.http.routers.qbittorrent.rule: Host(`qbt.falcon.nt`)
      traefik.http.services.qbittorrent.loadbalancer.server.port: 8080
      # Flood
      traefik.http.routers.flood.service: flood
      traefik.http.routers.flood.rule: Host(`flood.falcon.nt`)
      traefik.http.services.flood.loadbalancer.server.port: 3000
      <<: *pullio

  plex:
    image: hotio/plex
    container_name: plex
    restart: unless-stopped
    network_mode: host
    healthcheck:
      test: ["CMD", "curl", "localhost:32400/identity", "-fso", "/dev/null"]
    environment:
      <<: *env
    tmpfs:
      - /transcode:size=4g
    volumes:
      - /opt/appdata/plex:/config
      - /mnt/mfs-drive:/drive:ro
      - /mnt/knox:/knox:ro
    labels:
      traefik.http.services.plex.loadbalancer.server.port: 32400
      <<: *pullio

  tautulli:
    container_name: tautulli
    image: hotio/tautulli
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "localhost:8181", "-fso", "/dev/null"]
    depends_on:
      - plex
    environment:
      TP_HOTIO: true
      TP_THEME: plex
      <<: *env
    volumes:
      - /opt/appdata/tautulli:/config
      - /opt/appdata/plex/Logs:/plex-logs:ro
      - /opt/theme-park/tautulli:/etc/cont-init.d/98-themepark
    labels:
      <<: *pullio

  radarr:
    container_name: radarr
    image: hotio/radarr:nightly
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "localhost:7878", "-fso", "/dev/null"]
    ports:
      - 100.120.109.59:7878:7878
    environment:
      TP_HOTIO: true
      TP_THEME: onedark
      <<: *env
    volumes:
      - /opt/appdata/radarr:/config
      - /mnt/mfs-drive:/drive
      - /mnt/knox:/knox
      - /opt/theme-park/radarr:/etc/cont-init.d/98-themepark
    labels:
      <<: *pullio

  radarr4k:
    container_name: radarr4k
    image: hotio/radarr:nightly
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "localhost:7878", "-fso", "/dev/null"]
    ports:
      - 100.120.109.59:7879:7878
    environment:
      TP_HOTIO: true
      TP_THEME: organizr
      TP_ADDON: radarr-4k-logo
      <<: *env
    volumes:
      - /opt/appdata/radarr4k:/config
      - /mnt/mfs-drive:/drive
      - /mnt/knox:/knox
      - /opt/theme-park/radarr:/etc/cont-init.d/98-themepark
    labels:
      <<: *pullio

  sonarr:
    container_name: sonarr
    image: hotio/sonarr:nightly
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "localhost:8989", "-fso", "/dev/null"]
    ports:
      - 100.120.109.59:8989:8989
    environment:
      TP_HOTIO: true
      TP_THEME: plex
      <<: *env
    volumes:
      - /opt/appdata/sonarr:/config
      - /mnt/mfs-drive:/drive
      - /mnt/knox:/knox
      - /opt/theme-park/sonarr:/etc/cont-init.d/98-themepark
    labels:
      <<: *pullio

  sonarr4k:
    container_name: sonarr4k
    image: hotio/sonarr:nightly
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "localhost:8989", "-fso", "/dev/null"]
    ports:
      - 100.120.109.59:8990:8989
    environment:
      TP_HOTIO: true
      TP_THEME: organizr
      TP_ADDON: sonarr-4k-logo
      <<: *env
    volumes:
      - /opt/appdata/sonarr4k:/config
      - /mnt/mfs-drive:/drive
      - /mnt/knox:/knox
      - /opt/theme-park/sonarr:/etc/cont-init.d/98-themepark
    labels:
      <<: *pullio

  prowlarr:
    container_name: prowlarr
    image: hotio/prowlarr:nightly
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "localhost:9696", "-fso", "/dev/null"]
    environment:
      TP_HOTIO: true
      TP_THEME: aquamarine
      <<: *env
    volumes:
      - /opt/appdata/prowlarr:/config
      - /opt/theme-park/prowlarr:/etc/cont-init.d/98-themepark
    labels:
      <<: *pullio

#  tandoor:
#    container_name: tandoor
#    image: vabene1111/recipes
#    restart: unless-stopped
#    environment:
#      <<: *env
#    volumes:
#      - /opt/appdata/tandoor/static:/opt/recipes/staticfiles
#      - /opt/appdata/tandoor/media:/opt/recipes/mediafiles
#    labels:
#      <<: *pullio

#   postgres:
#     container_name: postgres
#     image: postgres
#     restart: unless-stopped
#     user: 1001:1001
#     shm_size: 128m
#     environment:
#       TZ: Asia/Kolkata
#     volumes:
#       - /opt/appdata/postgres/config.conf:/etc/postgresql/postgresql.conf
#       - /opt/appdata/postgres/data:/var/lib/postgresql/data
#     labels:
#       traefik.enable: false
#       <<: *pullio